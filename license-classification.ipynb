{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#IMPORTS\nimport os\nimport re\nimport nltk\nimport json\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport collections, functools, operator\nfrom tensorflow import keras\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom sklearn.utils import shuffle\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom transformers import BertTokenizer, TFBertModel, BertConfig, TFDistilBertModel, DistilBertTokenizer, DistilBertConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TEXT CORPUS CREATION\nCOMM_DIRECTORY = '/kaggle/input/licenses/Comm'\nNONCOMM_DIRECTORY = '/kaggle/input/licenses/NonC'\n\nstop_words = set(stopwords.words('english'))\n\ndef tokkenizer(directory):\n    text_corpus = ''\n    sentences = []\n    for file in os.listdir(directory):\n        with open(os.path.join(directory, file)) as json_file:\n            json_corpus = json.load(json_file)\n            temp_corpus = json_corpus['licenseText']\n            filt = r\"[\\n\\-\\=\\\\\\/\\t_`~¤•#\\xa0–—]\"\n            temp_corpus = re.sub(filt, ' ', temp_corpus)\n            temp_corpus = re.sub(r\" +\", ' ', temp_corpus)\n            sentences.append(temp_corpus)\n            text_corpus += temp_corpus\n\n    token_text = word_tokenize(text_corpus)      \n    token_text_stop = [w for w in token_text if not w.lower() in stop_words]\n    return token_text, token_text_stop, sentences, text_corpus\n\ncomm_tokens, comm_tokens_stop, comm_sentences, comm_corpus = tokkenizer(COMM_DIRECTORY)\nnoncomm_tokens, noncomm_tokens_stop, noncomm_senteces, noncomm_corpus = tokkenizer(NONCOMM_DIRECTORY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SENTIMENT ANALYSIS\ndef sentiment_analizer(corpus, text):\n    sia = SentimentIntensityAnalyzer()\n    comercial_corpus_sent = corpus.split('.')\n    scores = list(map(lambda x: sia.polarity_scores(x), comercial_corpus_sent))\n    result = dict(functools.reduce(operator.add, map(collections.Counter, scores)))\n    result = {key: value / len(scores) for key, value in result.items()}\n    print(text)\n    print(result)\n\nsentiment_analizer(comm_corpus, 'COMMERCIONAL')\nsentiment_analizer(noncomm_corpus, 'NONCOMMERCIONAL')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CREATE WORD CLOUD\ndef create_word_cloud(text):\n    comm_wordcloud = WordCloud(width = 800, height = 800,\n                    background_color ='white', stopwords={''},\n                    min_font_size = 10).generate(' '.join(text))\n\n    # plot the WordCloud image                      \n    plt.figure(figsize = (8, 8), facecolor = None)\n    plt.imshow(comm_wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n\n    plt.show()\n\ncreate_word_cloud(comm_tokens)\ncreate_word_cloud(noncomm_tokens)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words_modi = {'the', ',', 'of', '.', '-', 'to', 'this', 'in', 'that', 'a', '(', ')'}","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MOST COMMON WORDS AND DIFFERENCES BETWEEN THEM\ncomm_most_common = nltk.FreqDist(w.lower() for w in comm_tokens)\nnoncomm_most_common = nltk.FreqDist(w.lower() for w in noncomm_tokens)\ncomm_top = comm_most_common.most_common(100)\nnoncomm_top = noncomm_most_common.most_common(100)\n\ncomm_only_words = list(map(lambda x: re.sub(\"[0-9(),' \\\"]\",'' ,str(x)), comm_top))\nnoncomm_only_words = list(map(lambda x: re.sub(\"[0-9(),' \\\"]\",'' ,str(x)), noncomm_top))\n\ndiff = list(set(comm_only_words) - set(noncomm_only_words))\ndiff_comparision = [[w, comm_most_common[w]/len(comm_most_common), noncomm_most_common[w]/len(noncomm_most_common)] for w in diff]\n\nprint(diff)\nprint()\nprint(diff_comparision)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CREATING DATAFRAME\ncomm_df = pd.DataFrame()\ncomm_df['text'] = comm_sentences\ncomm_df['label'] = 1\n\nnoncomm_df = pd.DataFrame()\nnoncomm_df['text'] = noncomm_senteces\nnoncomm_df['label'] = 0\n\ndata = pd.concat([comm_df, noncomm_df])\ndata.reset_index(inplace=True)\ndata = shuffle(data)\nprint(data.head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#INITIALIZE DISTILBERT MODEL\ndbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MODEL DATA\nsentences = data['text']\nlabels = data['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CHOOSING CORRECT TOKENS LENGHT\ntokens_lenght = list(map(lambda x: len(dbert_tokenizer.tokenize(x)), sentences))\npercentil_50 = int(np.percentile(tokens_lenght, 50))\npercentil_75 = int(np.percentile(tokens_lenght, 75))\nmax_len = 512 #TODO increse lenght, beyond 512 model is crashing (https://stackoverflow.com/questions/60551906/tensorflow-huggingface-invalid-argument-indices0-624-624-is-not-in-0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MODEL CREATION\ndef create_model():\n    inpt = Input(shape=(max_len,), dtype='int64')\n    masks = Input(shape=(max_len,), dtype='int64')\n    dbert_layer = dbert_model(inpt, attention_mask=masks)[0][:,0,:]\n    dense = Dense(512,activation='relu')(dbert_layer)\n    dropout = Dropout(0.5)(dense)\n    pred = Dense(2, activation='sigmoid')(dropout) #or softmax\n    model = tf.keras.Model(inputs=[inpt, masks], outputs=pred)\n    print(model.summary())\n    return model\n\nmodel=create_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CREATING INPUT DATA\ninput_ids=[]\nattention_masks=[]\n\nfor sent in sentences:\n    dbert_inps=dbert_tokenizer.encode_plus(sent, add_special_tokens=True, max_length=max_len, pad_to_max_length=True, return_attention_mask=True, truncation=True)\n    input_ids.append(dbert_inps['input_ids'])\n    attention_masks.append(dbert_inps['attention_mask'])\n\ninput_ids=np.asarray(input_ids)\nattention_masks=np.array(attention_masks)\nlabels=np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input, test_input, train_label, test_label, train_mask, test_mask = train_test_split(input_ids, labels, attention_masks, test_size=0.2)\n\n#log_dir='dbert_model'\n#model_save_path='./dbert_model.h5'\n\n#callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n\nmodel.compile(loss=loss, optimizer=optimizer, metrics=[metric])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([train_input, train_mask], train_label, batch_size=16, epochs=50, validation_data=([test_input, test_mask], test_label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}